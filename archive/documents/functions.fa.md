[raed in english](functions.md)

### کلاس ها و توابع 

## `webScriper.py` : 
# مستندات کلاس DigiKalaScraper

کلاس `DigiKalaScraper` برای تسهیل عملیات وب اسکرپینگ در وب‌سایت دیجی‌کالا طراحی شده است. این کلاس با استفاده از اجزای مختلف، اطلاعات مربوط به فروشندگان، محصولات و جزئیات محصولات را استخراج کرده و این داده‌ها را در یک پایگاه داده ذخیره می‌کند. این کلاس همچنین امکاناتی برای صدور داده‌ها به فایل‌های CSV و مدیریت فرآیند وب اسکرپینگ از طریق حالت‌های مختلف را فراهم می‌آورد.

# مقدمه

```python
DigiKalaScraper(config_file_path, log)
```
  - `config_file_path`: مسیر فایل پیکربندی. این فایل تعیین می‌کند که اسکرپر در حالت کنسول یا وب کار کند.
  - `log`: نمونه Logger برای ثبت پیام‌ها در طول فرآیند اسکرپینگ.
  
# متدها 

`_initialize_settings()`

تنظیمات اسکرپر را از فایل پیکربندی مقداردهی اولیه می‌کند. تنظیمات شامل مسیر پایگاه داده، مسیر وب درایور، گزینه حالت بی‌سر و نوع درایور است.

`initialize_driver(geko_path, driver_type, headless_mode, db_handler, logger)`

وب درایور را برای فرآیند اسکرپینگ مقداردهی اولیه می‌کند.
  - `geko_path`: مسیر GeckoDriver.
  - `driver_type`: نوع وب درایور (firefox یا chrome).
  - `headless_mode`: بولین که نشان می‌دهد آیا مرورگر باید در حالت بی‌سر اجرا شود.
  - `db_handler`: نمونه کلاس DataBaseHandler.
  - `logger`: نمونه Logger.

`get_sellers()`

لیست فروشندگان را از پایگاه داده بازیابی می‌کند.

`show_sellers()`

تمام فروشندگان موجود در پایگاه داده را نمایش می‌دهد و اجازه می‌دهد کاربر یک فروشنده را برای اسکرپینگ محصولاتش انتخاب کند.

`check_crawl_url(mode, input_url)`

URL ورودی را بر اساس حالت اسکرپینگ مشخص شده اعتبارسنجی می‌کند.

  - `mode`: حالت اسکرپینگ (SingleProductCrawlMode, SingleSellerCrawlMode, CategoryCrawlMode).
  - `input_url`: URL برای اعتبارسنجی.

`initialize_crawl_for_products(available_products)`

فرآیند اسکرپینگ را برای لیستی از URL‌های محصول مقداردهی اولیه می‌کند.

  - `available_products`: لیست URL‌های محصول برای اسکرپینگ.

`execute_crawl(mode, input_url, scroll_count, seller_info=None)`

فرآیند وب اسکرپینگ را بر اساس حالت و تنظیمات مشخص شده اجرا می‌کند.

  - `mode`: حالت اسکرپینگ.
  - `input_url`: URL نقطه شروع اسکرپ.
  - `scroll_count`: تعداد دفعات اسکرول صفحه (در صورت لزوم).
  - `seller_info`: اطلاعات اختیاری فروشنده برای اسکرپینگ هدفمند.

`remove_old_file(filename)`

یک فایل قدیمی را اگر وجود دارد حذف می‌کند.

  - `filename`: نام فایلی که باید حذف شود.

`save_to_csv(data, headers, filename)`

داده‌ها را به یک فایل CSV صادر می‌کند.

  - `data`: داده‌هایی که باید صادر شوند.
  - `headers`: سرصفحه‌های ستون برای فایل CSV.
  - `filename`: نام فایل CSV.

`export_table(table_name, seller_id=None, condition=None)`

داده‌ها از یک جدول مشخص را به یک فایل CSV صادر می‌کند.

  - `table_name`: نام جدول پایگاه داده.
  - `seller_id`: شناسه فروشنده اختیاری برای فیلتر کردن داده‌ها.
  - `condition`: شرایط اضافی برای فیلتر کردن داده‌ها.

`export_data_to_csv(export_mode, seller_id=None, seller_name=None)`

صدور داده‌ها به فایل‌های CSV را بر اساس حالت مشخص شده مدیریت می‌کند.

  - `export_mode`: حالت صدور داده (all_seller, seller_products و غیره).
  - `seller_id`: شناسه فروشنده اختیاری برای صدور هدفمند.
  - `seller_name`: نام فروشنده اختیاری برای صدور هدفمند.

`database_report()`

گزارشی از محتوای پایگاه داده تولید می‌کند، شامل تعداد فروشندگان، محصولات و اطلاعات استخراج شده.
نمونه استفاده

```python
logger = setup_logger()
scraper = DigiKalaScraper(config_file_path="config/console_config.ini", log=logger)
scraper.execute_crawl(mode="SingleProductCrawlMode", input_url="https://www.digikala.com/product/dkp-12345", scroll_count=3)
```


## `product_details_extractor.py` : 



# مستندات کلاس ProductDetailsExtractor

کلاس `ProductDetailsExtractor` برای استخراج جزئیات محصول از یک صفحه HTML داده شده طراحی شده است.

## مقدمه

```python
ProductDetailsExtractor(driver, db_handler, log)
```
  - `driver`: نمونه درایور وب برای بارگذاری صفحات وب و انجام عملیات‌های مرورگر.
  - `db_handler`: نمونه از کلاس `DataBaseHandler` برای انجام عملیات‌های پایگاه داده.
  - `log`: نمونه از کلاس `Logger` برای ثبت پیام‌ها.

## متدها 

`clean_text(text)`

این متد برای حذف کاراکترهای ناخواسته از متن و بازگرداندن متن پاک‌سازی شده استفاده می‌شود.

`check_with_multi_class_name(element, field_name, tag_name, attrs_name, attrs_list)`

این متد برای یافتن یک عنصر با استفاده از نام‌های کلاس چندگانه استفاده می‌شود.

`safe_find(soup, finds, tag, attrs)`

این متد برای انجام عملیات یافتن عناصر با استفاده از BeautifulSoup به صورت امن (با استفاده از try و catch) است.

`safe_extraction(element_name, element, extraction_function)`

این متد برای فراخوانی یک تابع استخراج با ارسال آرگومان‌های مورد نیاز و مدیریت خطاها به صورت امن است.

`product_elements_extraction(soup)`

این متد برای استخراج تمام عناصر مورد نیاز محصولات از یک صفحه HTML با استفاده از کتابخانه BeautifulSoup است.

`main_product_details_extraction(element)`

این متد برای استخراج جزئیات اصلی محصول از یک عنصر div است.

`product_buy_box_extraction(element)`

این متد برای استخراج جزئیات جعبه خرید محصول از یک عنصر div است.

`product_image_extraction(element)`

این متد برای استخراج URL تصاویر محصول از برچسب‌های img در صفحه HTML است.

`other_seller_box_extraction(element)`

این متد برای استخراج اطلاعات فروشندگان دیگری که این محصول را دارند، از یک عنصر HTML است.

`similar_products_extraction(element)`

این متد برای استخراج اطلاعات محصولات مشابه از صفحه توضیحات محصول است.

`related_videos_extraction(element)`

این متد برای استخراج اطلاعات ویدیوهای مرتبط با مرور محصول است.

`expert_check_box_extraction(element)`

این متد برای استخراج اطلاعات جعبه بررسی تخصصی از صفحه است.

`specifications_box_extraction(element)`

این متد برای استخراج اطلاعات مشخصات محصول از صفحه است.

`reviews_box_extraction(element)`

این متد برای استخراج داده‌های نقد و بررسی از صفحه است.

`question_box_extraction(element)`

این متد برای استخراج داده‌های سوال و پاسخ از صفحه است.

`also_bought_items_extraction(element)`

این متد برای استخراج آیتم‌هایی که همراه با محصول اصلی خریداری شده‌اند، است.

`seller_offer_extraction(element)`

این متد برای استخراج پیشنهادهای فروشنده از صفحه توضیحات محصول است.

`page_extraction(product_id, product_url)`

این متد برای استخراج تمام داده‌های مرتبط از یک صفحه وب مشخص است.

`check_not_empty(data)`

این متد برای بررسی اینکه آیا هر فیلدی خالی است یا خیر استفاده می‌شود.

`run(url)`

این متد برای شروع فرآیند استخراج داده‌ها از یک URL محصول خاص است.

## `seller_product_data_extractor.py` : 

# مستندات کلاس SellerProductDataExtractor

کلاس `SellerProductDataExtractor` برای استخراج داده‌های مورد نیاز از لیست محصولات یک فروشنده استفاده می‌شود.

## سازنده

```python
SellerProductDataExtractor(driver, db_handler, log)
```

  - `driver`: نمونه درایور برای دسترسی به صفحات وب.
  - `db_handler`: نمونه کنترل‌کننده پایگاه داده برای انجام عملیات‌های مرتبط با داده.
  - `log`: نمونه‌ای برای ثبت رویدادها و اطلاعات مرتبط با فرآیند استخراج داده.

## متدها

`has_desired_text(tags, find_text)`

بررسی می‌کند که آیا متن مورد نظر در تگ‌های داده شده وجود دارد یا خیر.

`seller_details(soup)`

جزئیات فروشنده را از یک شیء BeautifulSoup استخراج و بازمی‌گرداند.

`extract_product_details(product)`

اطلاعات مربوط به یک محصول (قیمت، قیمت با تخفیف، توضیحات) را از یک عنصر محصول استخراج می‌کند.

`check_category(url, scroll_count)`

داده‌های مربوط به یک دسته بندی محصولات را از صفحه دیجیکالا استخراج می‌کند.

`check_seller(url)`

جزئیات یک فروشنده خاص را با استفاده از آدرس URL استخراج می‌کند.


## `driver_manager.py` : 

کلاس `DriverManager` برای اولیه‌سازی و مدیریت درایور مرورگر وب و استخراج داده‌ها از صفحات وب استفاده می‌شود.

`__init__(self, driver_path, log, headless_mode, driver_type)`
این متد برای اولیه‌سازی درایور مرورگر وب استفاده می‌شود.

  - `driver_path`: مسیر درایور مرورگر
  - `log`: شیء برای ثبت رویدادها
  - `headless_mode`: حالت بدون سر (اجرای مرورگر بدون نمایش ظاهری)
  - `driver_type`: نوع مرورگر (firefox یا chrome)

`initialize_driver(self, headless_mode)`

این متد برای اولیه‌سازی درایور مرورگر بر اساس نوع مرورگر و حالت headless انتخابی استفاده می‌شود.

`load_page(self, url)`

این متد برای بارگذاری یک صفحه وب با URL مشخص شده و تنظیم آن به عنوان صفحه فعال برای اقدامات بعدی استفاده می‌شود.

`get_page_source(self)`

این متد برای بازگرداندن کد منبع صفحه فعلی به عنوان شیء BeautifulSoup استفاده می‌شود.

`get_prdoucts_on_page(self, page_source, return_value)`

این متد برای تجزیه محتوای HTML داده شده و یافتن محصولات در یک صفحه وب استفاده می‌شود.

`get_seller_id(self)`

این متد برای استخراج شناسه فروشنده از صفحه جزئیات محصول استفاده می‌شود.

`close_driver(self)`

این متد برای بستن درایور استفاده می‌شود.


## `db_handler.py` : 

`__init__(self, db_path, log)`

این متد برای مقداردهی اولیه به پایگاه داده و برقراری ارتباط با آن استفاده می‌شود.

  - `db_path`: مسیر فایل پایگاه داده
  - `log`: شیء برای ثبت وقایع


`create_tables(self)`

این متد جداول مورد نیاز در پایگاه داده را ایجاد می‌کند اگر از قبل وجود نداشته باشند.

`get_next_id(self, table_name, fields_id, id_name)`

این متد برای گرفتن شناسه بعدی برای افزایش خودکار در SQLite استفاده می‌شود.

`get_row_info(self, fields, table_name, condition=None, return_as_list=False)`

این متد برای استخراج اطلاعات از پایگاه داده بر اساس شرایط مشخص و فیلدهای درخواستی توسط کاربر است.

`get_connection(self)`

این متد برای برقراری ارتباط با پایگاه داده استفاده می‌شود.

`get_sellers(self)`

این متد فروشندگان را از پایگاه داده استخراج می‌کند.

`get_column_names(self, table_name)`

این متد نام ستون‌های یک جدول مشخص را برمی‌گرداند.

`check_field_value(self, row_data, crawl_data)`

این متد بررسی می‌کند که آیا یک فیلد خاص پر شده است یا خیر.

`check_existing_data(self, row_id, column_name, table_name)`

این متد برای بررسی فیلدها در پایگاه داده استفاده می‌شود که آیا به‌روز شده‌اند یا خیر.

`parse_json_fields(self, record)`

این متد فیلدها را به json تجزیه می‌کند.

`replace_recode_to_history_table(self, data, column_name, table_name)`

این متد داده‌های قدیمی را از جدول اصلی به جدول تاریخچه منتقل می‌کند.

`insert_recode_to_table(self, data, table_name)`

این متد یک خط جدید را در جدول مشخص شده اضافه می‌کند یا یک خط موجود را به‌روزرسانی می‌کند.

`update_database(self, data, column_name, table_name)`

این متد رکوردها در یک جدول را بر اساس یک ستون به‌روزرسانی می‌کند.

`close_connection(self)`

این متد ارتباط با پایگاه داده را می‌بندد.

## `logger.py` : 

تابع `setup_logger`

این تابع برای تنظیم کننده لاگر کلی برنامه استفاده می‌شود.
بازگشت:

  - شیء لاگر

نحوه کار:

  - یک شیء لاگر با نام "DigikalaCrawler" ایجاد می‌کند.
  - سطح لاگر را به DEBUG تنظیم می‌کند.
  - یک handler برای نمایش خروجی لاگ در کنسول اضافه می‌کند.
  - یک formatter برای تعیین فرمت پیام‌های لاگ ایجاد و به handler اضافه می‌کند.

تابع `web_setup_logger`

این تابع برای تنظیم کننده لاگر مخصوص بخش وب برنامه استفاده می‌شود.
بازگشت:

  - شیء لاگر

نحوه کار:

  - یک شیء لاگر با نام "DigikalaCrawler" ایجاد می‌کند.
  - سطح لاگر را به DEBUG تنظیم می‌کند و انتشار آن را غیرفعال می‌کند تا از تکرار لاگ‌ها جلوگیری شود.
  - یک handler فایلی برای ذخیره لاگ‌ها در یک فایل لاگ ایجاد می‌کند.
  - فرمت‌بندی مشخصی برای پیام‌های لاگ تعیین و به handler فایلی اضافه می‌کند.

این توابع به منظور ثبت و نگهداری اطلاعات مربوط به فعالیت‌ها و خطاهای برنامه در حین اجرا استفاده می‌شوند تا تجزیه و تحلیل رویدادها و عیب‌یابی آسان‌تر شود.


## `console_panel.py` : 

تابع __init__

این تابع اولیه‌سازی برای پنل کنسول انجام می‌دهد و یک نمونه از DigiKalaScraper را با استفاده از لاگر و مسیر فایل پیکربندی مشخص شده، ایجاد می‌کند.

تابع `menus`

این تابع منوهای مختلف را برای انتخاب عملیات‌های مختلف در خزنده تعریف می‌کند، از جمله خزش برای یک دسته‌بندی، خزش برای یک فروشنده مشخص، و خروجی گرفتن به فرمت CSV.

تابع `crawl_options`

این تابع بر اساس گزینه‌ای که کاربر انتخاب می‌کند، عملیات مربوط به خزش را اجرا می‌کند. هر گزینه به یک مد خاص از خزش مربوط می‌شود و توابع مربوطه را فراخوانی می‌کند.

تابع `show_help`

این تابع راهنمایی‌هایی در مورد نحوه استفاده از خزنده ارائه می‌دهد و به کاربر اطلاعاتی درباره پیکربندی و اجرای عملیات‌های مختلف می‌دهد.

تابع `reconfig`

این تابع امکان تنظیم مجدد پیکربندی خزنده را فراهم می‌کند، در صورت نیاز به تغییر تنظیمات اولیه.

تابع `show_examples`

این تابع نمونه‌هایی از URL‌هایی که می‌توان برای خزش استفاده کرد را نمایش می‌دهد، تا کاربر بتواند به راحتی URL‌های معتبر را وارد کند.

تابع `csv_export`

این تابع امکان خروجی گرفتن داده‌های جمع‌آوری شده به فرمت CSV را فراهم می‌کند.

تابع `database_report_show`

این تابع گزارشی جامع از داده‌های ذخیره شده در دیتابیس را ارائه می‌دهد، شامل تعداد رکوردهای موجود در جداول مختلف.

تابع `get_crawl_input`

این تابع ورودی‌های لازم برای شروع عملیات خزش را از کاربر دریافت می‌کند.

تابع `show_menu`

این تابع منوی انتخاب‌های مختلف را به کاربر نمایش می‌دهد و امکان انتخاب گزینه‌های مختلف را فراهم می‌کند.

تابع `run`

این تابع اصلی برای اجرای پنل کنسول است و منطق اصلی برنامه را کنترل می‌کند، شامل دریافت ورودی‌ها و اجرای عملیات‌های مختلف.

این کلاس به کاربر امکان می‌دهد از طریق یک پنل کنسولی ساده، عملیات‌های مختلفی را بر روی خزنده دیجی‌کالا اجرا کند و داده‌های مورد نظر را جمع‌آوری کند.


## `app.py` : 

این کد یک رابط وب برای خزنده دیجی‌کالا تنظیم می‌کند که به کاربران اجازه می‌دهد از طریق مرورگر وب، خزنده را کنترل کنند.

### مقدمه

کلاس `WebGUIApp` با مسیر فایل پیکربندی و یک logger ایجاد می‌شود. یک برنامه Flask ایجاد کرده و مسیرهایی برای عملکردهای مختلف خزنده تعریف می‌کند.
تعریف مسیرها

  
  - مسیر / صفحه اصلی را نمایش می‌دهد که کاربران می‌توانند از آنجا انواع مختلفی از خزنده‌ها را شروع کنند یا به تنظیمات بروند.
  - مسیر `/settings` به کاربران امکان می‌دهد تا تنظیمات خزنده را از طریق فرم وب پیکربندی کنند.
  - مسیر `/get-logs` آخرین لاگ‌ها از خزنده را نمایش می‌دهد.
  - مسیرهایی مانند `/start-category-crawl`, `/start_single_seller`, `/start_single_product`, و `/all_products` نقاط پایانی برای شروع انواع خاصی از خزنده‌ها بر اساس ورودی کاربر از رابط وب هستند.
  - مسیرهای صادراتی (`/export_all_seller_data`, `/export_seller_products_id`, `/export_all_products` و غیره) به کاربران امکان می‌دهند داده‌های جمع‌آوری شده را به
  فایل‌های CSV صادر کنند.
  - مسیر `/report` گزارش جامعی از پایگاه داده ارائه می‌دهد که تعداد رکوردها در جداول مختلف را نشان می‌دهد.

### متدها

  - متد `add_routes` تمام نقاط پایانی و عملکردهای آنها را تعریف می‌کند.
  - `crawl_options` وظایف خزنده خاصی را بر اساس حالت انتخابی توسط کاربر از طریق رابط وب اجرا می‌کند.
  - متد `run` برنامه Flask را شروع می‌کند و رابط وب را از طریق مرورگر قابل دسترسی می‌کند.

این تنظیمات، یک رابط کاربری وب آسان برای تعامل با خزنده دیجی‌کالا فراهم می‌کند و دسترسی به آن را برای کاربران بدون نیاز به تعامل مستقیم با کنسول یا کد، آسان‌تر می‌کند.


## `TODOS.py` : 

این فایل، یک برنامه‌ریزی و ردیابی کارهایی است که باید بر روی پروژه خزنده دیجی‌کالا انجام شود، شامل به‌روزرسانی‌ها، اصلاحیه‌ها و اضافه‌های برنامه‌ریزی شده.

اسناد / README

  - به‌روزرسانی README برای نسخه جدید با رابط گرافیکی کاربر (GUI) مد نظر است.
  - اسناد به‌روزرسانی شوند تا نحوه کار این پروژه و امکانات آن را نمایش دهند.

پیکربندی

  - افزودن Docker به پروژه و اضافه کردن واحدهای تست برنامه‌ریزی شده است.

پنل‌ها

  - بررسی اتصال اینترنت پیش از شروع پنل.
  - در پنل وب، مشکلی وجود دارد که گزارش‌ها در آغاز دوبار چاپ می‌شوند اما در ادامه برنامه تنها یکبار نمایش داده می‌شوند.
  - در پنل کنسول، گزینه‌ای برای بازنشانی تنظیمات و پیکربندی مجدد اضافه شده است.

db_handler.py

  - اصلاح شده که تابع `get_next_id` باید همان ایدی را هنگام جایگزینی داده‌ها با جدول تاریخی ارائه دهد.

driver_manager.py

  - بررسی صفحات 404 و 503 و صفحات محصولات غیرموجود پیش از دریافت منبع صفحه.
  - قبل از دریافت منبع صفحه در `driver_manager،` باید بررسی شود که آیا محصولات در حال بارگذاری هستند.

logger
products_details_extractor.py

  - در حال حاضر، 20 آیتم برای بررسی‌ها و سوالات دریافت می‌شوند. برای دریافت آیتم‌های بیشتر، در صورت وجود، باید اقداماتی انجام شود.

seller_product_data_extractor.py
webScraper.py

  - الگوهای regex نیاز به به‌روزرسانی دارند تا بررسی کنند آیا لینک معتبر است.
  - نام‌های حالت‌ها باید در تمام فایل‌های اسکریپت به‌روزرسانی شوند.

این فایل، یک دید کلی به وظایف پیش رو، اصلاحیه‌های انجام شده و برنامه‌های آتی برای بهبود و توسعه خزنده دیجی‌کالا ارائه می‌دهد.